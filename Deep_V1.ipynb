{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5538031f-aaa0-4ef3-b558-5500124f9e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linh-dinh-1012/projet_fil_rouge/Projet_fil_rouge_2024/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, classification_report\n",
    "import shap\n",
    "from skorch import NeuralNetClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc1ea127-b9ee-4319-aac0-de35070c8e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ 1) Chargement et Pr√©traitement des Donn√©es ============\n",
    "path_plm = \"data/matricePLM_genes.txt\"\n",
    "df_plm = pd.read_csv(path_plm, sep=\"\\t\", index_col=0)\n",
    "\n",
    "path_expr = \"data/expression_final.csv\"\n",
    "df_expr = pd.read_csv(path_expr, index_col=0)\n",
    "\n",
    "# Alignement des donn√©es\n",
    "genes_communs = df_plm.index.intersection(df_expr.index)\n",
    "df_plm = df_plm.loc[genes_communs].sort_index()\n",
    "df_expr = df_expr.loc[genes_communs].sort_index()\n",
    "\n",
    "# Conversion des donn√©es en tenseurs PyTorch\n",
    "plm_matrix = torch.tensor(df_plm.values, dtype=torch.float32)\n",
    "df_expr.replace({-1: 2}, inplace=True)  # Remplacer -1 par 2 pour √©viter l'erreur avec CrossEntropyLoss\n",
    "labels = torch.tensor(df_expr.values, dtype=torch.long)\n",
    "stress_conditions = torch.arange(df_expr.shape[1])  # Liste des conditions de stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "288d6ebe-fe21-41eb-8628-21f85c8b8688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ 2) D√©finition de la Classe Dataset ============\n",
    "class GeneExpressionDataset(Dataset):\n",
    "    def __init__(self, plm_matrix, stress_conditions, labels):\n",
    "        self.plm_matrix = plm_matrix\n",
    "        self.stress_conditions = stress_conditions\n",
    "        self.labels = labels\n",
    "        self.num_genes = plm_matrix.size(0)\n",
    "        self.num_stress = stress_conditions.size(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_genes * self.num_stress\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gene_idx = idx // self.num_stress\n",
    "        stress_idx = idx % self.num_stress\n",
    "        plm_vector = self.plm_matrix[gene_idx]\n",
    "        stress_id = self.stress_conditions[stress_idx]\n",
    "        label = self.labels[gene_idx, stress_idx]\n",
    "        return plm_vector, stress_id, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a34aaec-052b-440f-a7dc-fd951ab2706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ 3) D√©finition du Mod√®le ============\n",
    "class GeneExpressionModel(nn.Module):\n",
    "    def __init__(self, num_plms, embedding_dim=16, hidden_dim=64, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.plm_embedding = nn.Embedding(num_embeddings=num_plms, embedding_dim=embedding_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, plm_indices_batch):\n",
    "        plm_embedded_batch = []\n",
    "        for indices in plm_indices_batch:\n",
    "            if indices.numel() == 0:\n",
    "                aggregated = torch.zeros(self.plm_embedding.embedding_dim, device=indices.device)\n",
    "            else:\n",
    "                embedded = self.plm_embedding(indices)\n",
    "                aggregated = embedded.mean(dim=0)\n",
    "            plm_embedded_batch.append(aggregated)\n",
    "        aggregated_plm = torch.stack(plm_embedded_batch, dim=0)\n",
    "        return self.mlp(aggregated_plm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53bc8e6b-ac4e-4680-9202-ecd5f0ab8262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ 4) Pr√©paration pour l'Entra√Ænement ============\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "learning_rate = 5e-4\n",
    "\n",
    "dataset = GeneExpressionDataset(plm_matrix, stress_conditions, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "num_plms = plm_matrix.size(1)\n",
    "model = GeneExpressionModel(num_plms)\n",
    "\n",
    "# Calculer le nombre d'√©chantillons de chaque classe\n",
    "num_samples = torch.bincount(labels.flatten(), minlength=3)  # D√©calage -1 √† 0, 0 √† 1, 1 √† 2\n",
    "class_weights = 1.0 / num_samples.float()\n",
    "class_weights /= class_weights.sum() # Normaliser la somme des poids √† 1\n",
    "\n",
    "# Utiliser la perte d'entropie crois√©e pond√©r√©e\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(plm_matrix.device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c3ec246-04f6-4169-ad0a-b7ba67d6392f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 1.0903\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0903)\n",
      "Epoch 2/100, Loss: 1.0833\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0833)\n",
      "Epoch 3/100, Loss: 1.0785\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0785)\n",
      "Epoch 4/100, Loss: 1.0757\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0757)\n",
      "Epoch 5/100, Loss: 1.0737\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0737)\n",
      "Epoch 6/100, Loss: 1.0721\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0721)\n",
      "Epoch 7/100, Loss: 1.0710\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0710)\n",
      "Epoch 8/100, Loss: 1.0694\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0694)\n",
      "Epoch 9/100, Loss: 1.0687\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0687)\n",
      "Epoch 10/100, Loss: 1.0680\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0680)\n",
      "Epoch 11/100, Loss: 1.0672\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0672)\n",
      "Epoch 12/100, Loss: 1.0667\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0667)\n",
      "Epoch 13/100, Loss: 1.0661\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0661)\n",
      "Epoch 14/100, Loss: 1.0656\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0656)\n",
      "Epoch 15/100, Loss: 1.0652\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0652)\n",
      "Epoch 16/100, Loss: 1.0647\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0647)\n",
      "Epoch 17/100, Loss: 1.0642\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0642)\n",
      "Epoch 18/100, Loss: 1.0642\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0642)\n",
      "Epoch 19/100, Loss: 1.0642\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0642)\n",
      "Epoch 20/100, Loss: 1.0639\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0639)\n",
      "Epoch 21/100, Loss: 1.0634\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0634)\n",
      "Epoch 22/100, Loss: 1.0635\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 1 epoch(s)\n",
      "Epoch 23/100, Loss: 1.0630\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0630)\n",
      "Epoch 24/100, Loss: 1.0628\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0628)\n",
      "Epoch 25/100, Loss: 1.0625\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0625)\n",
      "Epoch 26/100, Loss: 1.0626\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 1 epoch(s)\n",
      "Epoch 27/100, Loss: 1.0623\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0623)\n",
      "Epoch 28/100, Loss: 1.0624\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 1 epoch(s)\n",
      "Epoch 29/100, Loss: 1.0618\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0618)\n",
      "Epoch 30/100, Loss: 1.0621\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 1 epoch(s)\n",
      "Epoch 31/100, Loss: 1.0616\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0616)\n",
      "Epoch 32/100, Loss: 1.0619\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 1 epoch(s)\n",
      "Epoch 33/100, Loss: 1.0619\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 2 epoch(s)\n",
      "Epoch 34/100, Loss: 1.0616\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0616)\n",
      "Epoch 35/100, Loss: 1.0616\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0616)\n",
      "Epoch 36/100, Loss: 1.0614\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0614)\n",
      "Epoch 37/100, Loss: 1.0616\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 1 epoch(s)\n",
      "Epoch 38/100, Loss: 1.0613\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0613)\n",
      "Epoch 39/100, Loss: 1.0613\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 1 epoch(s)\n",
      "Epoch 40/100, Loss: 1.0610\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0610)\n",
      "Epoch 41/100, Loss: 1.0612\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 1 epoch(s)\n",
      "Epoch 42/100, Loss: 1.0610\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 2 epoch(s)\n",
      "Epoch 43/100, Loss: 1.0608\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0608)\n",
      "Epoch 44/100, Loss: 1.0608\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 1 epoch(s)\n",
      "Epoch 45/100, Loss: 1.0609\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 2 epoch(s)\n",
      "Epoch 46/100, Loss: 1.0608\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0608)\n",
      "Epoch 47/100, Loss: 1.0609\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 1 epoch(s)\n",
      "Epoch 48/100, Loss: 1.0608\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 2 epoch(s)\n",
      "Epoch 49/100, Loss: 1.0607\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0607)\n",
      "Epoch 50/100, Loss: 1.0607\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 1 epoch(s)\n",
      "Epoch 51/100, Loss: 1.0605\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0605)\n",
      "Epoch 52/100, Loss: 1.0605\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0605)\n",
      "Epoch 53/100, Loss: 1.0604\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0604)\n",
      "Epoch 54/100, Loss: 1.0605\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 1 epoch(s)\n",
      "Epoch 55/100, Loss: 1.0609\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 2 epoch(s)\n",
      "Epoch 56/100, Loss: 1.0603\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0603)\n",
      "Epoch 57/100, Loss: 1.0607\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 1 epoch(s)\n",
      "Epoch 58/100, Loss: 1.0609\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 2 epoch(s)\n",
      "Epoch 59/100, Loss: 1.0602\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0602)\n",
      "Epoch 60/100, Loss: 1.0606\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 1 epoch(s)\n",
      "Epoch 61/100, Loss: 1.0606\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 2 epoch(s)\n",
      "Epoch 62/100, Loss: 1.0604\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 3 epoch(s)\n",
      "Epoch 63/100, Loss: 1.0603\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 4 epoch(s)\n",
      "Epoch 64/100, Loss: 1.0603\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 5 epoch(s)\n",
      "Epoch 65/100, Loss: 1.0602\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0602)\n",
      "Epoch 66/100, Loss: 1.0602\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0602)\n",
      "Epoch 67/100, Loss: 1.0603\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 1 epoch(s)\n",
      "Epoch 68/100, Loss: 1.0602\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 2 epoch(s)\n",
      "Epoch 69/100, Loss: 1.0606\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 3 epoch(s)\n",
      "Epoch 70/100, Loss: 1.0600\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0600)\n",
      "Epoch 71/100, Loss: 1.0602\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 1 epoch(s)\n",
      "Epoch 72/100, Loss: 1.0603\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 2 epoch(s)\n",
      "Epoch 73/100, Loss: 1.0600\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 3 epoch(s)\n",
      "Epoch 74/100, Loss: 1.0604\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 4 epoch(s)\n",
      "Epoch 75/100, Loss: 1.0598\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0598)\n",
      "Epoch 76/100, Loss: 1.0598\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0598)\n",
      "Epoch 77/100, Loss: 1.0601\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 1 epoch(s)\n",
      "Epoch 78/100, Loss: 1.0601\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 2 epoch(s)\n",
      "Epoch 79/100, Loss: 1.0601\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 3 epoch(s)\n",
      "Epoch 80/100, Loss: 1.0600\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 4 epoch(s)\n",
      "Epoch 81/100, Loss: 1.0598\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 5 epoch(s)\n",
      "Epoch 82/100, Loss: 1.0600\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 6 epoch(s)\n",
      "Epoch 83/100, Loss: 1.0597\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0597)\n",
      "Epoch 84/100, Loss: 1.0597\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 1 epoch(s)\n",
      "Epoch 85/100, Loss: 1.0598\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 2 epoch(s)\n",
      "Epoch 86/100, Loss: 1.0601\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 3 epoch(s)\n",
      "Epoch 87/100, Loss: 1.0598\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 4 epoch(s)\n",
      "Epoch 88/100, Loss: 1.0602\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 5 epoch(s)\n",
      "Epoch 89/100, Loss: 1.0603\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 6 epoch(s)\n",
      "Epoch 90/100, Loss: 1.0600\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 7 epoch(s)\n",
      "Epoch 91/100, Loss: 1.0596\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0596)\n",
      "Epoch 92/100, Loss: 1.0602\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 1 epoch(s)\n",
      "Epoch 93/100, Loss: 1.0597\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 2 epoch(s)\n",
      "Epoch 94/100, Loss: 1.0599\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 3 epoch(s)\n",
      "Epoch 95/100, Loss: 1.0598\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 4 epoch(s)\n",
      "Epoch 96/100, Loss: 1.0595\n",
      "‚úîÔ∏è Meilleur mod√®le sauvegard√© √† checkpoints/best_model.pth (Loss: 1.0595)\n",
      "Epoch 97/100, Loss: 1.0599\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 1 epoch(s)\n",
      "Epoch 98/100, Loss: 1.0597\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 2 epoch(s)\n",
      "Epoch 99/100, Loss: 1.0602\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 3 epoch(s)\n",
      "Epoch 100/100, Loss: 1.0602\n",
      "‚ö†Ô∏è Pas d'am√©lioration depuis 4 epoch(s)\n"
     ]
    }
   ],
   "source": [
    "# ============ 5) Boucle d'Entra√Ænement avec Checkpoint et Early Stopping ============\n",
    "best_loss = float('inf')\n",
    "checkpoint_path = \"checkpoints/best_model.pth\"\n",
    "patience = 10\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for plm_batch, stress_id_batch, label_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        plm_indices_batch = [(plm_vec > 0).nonzero(as_tuple=True)[0] for plm_vec in plm_batch]\n",
    "        outputs = model(plm_indices_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True) \n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"‚úîÔ∏è Meilleur mod√®le sauvegard√© √† {checkpoint_path} (Loss: {best_loss:.4f})\")\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"‚ö†Ô∏è Pas d'am√©lioration depuis {counter} epoch(s)\")\n",
    "    \n",
    "    if counter >= patience:\n",
    "        print(\"üõë Early stopping d√©clench√© !\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e9bbfde-b0d6-46d5-a92d-833c785cfdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Mod√®le final sauvegard√© √† checkpoints/final_model.pth\n"
     ]
    }
   ],
   "source": [
    "# ============ 6) Sauvegarde du Mod√®le ==========\n",
    "final_model_path = \"checkpoints/final_model.pth\"\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"üìå Mod√®le final sauvegard√© √† {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0449bdc2-e8f5-4200-9283-a3ad813c5f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.7621162572346742}\n"
     ]
    }
   ],
   "source": [
    "# ============ 7) √âvaluation du Mod√®le ============\n",
    "def evaluate_model(model, dataloader, criterion=None):\n",
    "    model.eval()\n",
    "    all_preds, all_labels, total_loss = [], [], 0.0\n",
    "    with torch.no_grad():\n",
    "        for plm_batch, stress_id_batch, label_batch in dataloader:\n",
    "            plm_indices_batch = [(plm_vec > 0).nonzero(as_tuple=True)[0] for plm_vec in plm_batch]\n",
    "            outputs = model(plm_indices_batch)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(label_batch.cpu().numpy())\n",
    "            if criterion:\n",
    "                loss = criterion(outputs, label_batch)\n",
    "                total_loss += loss.item()\n",
    "    return {\"Accuracy\": accuracy_score(all_labels, all_preds)}\n",
    "\n",
    "test_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "results = evaluate_model(model, test_dataloader, criterion)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e613d856-5898-4fe5-afc0-25058df4d3f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plm_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model(plm_indices_batch)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Conversion de la matrice PLM en tableau NumPy pour SHAP\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m plm_matrix_np \u001b[38;5;241m=\u001b[39m \u001b[43mplm_matrix\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Initialisation de l'explainer SHAP avec KernelExplainer\u001b[39;00m\n\u001b[1;32m     13\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mKernelExplainer(model_wrapper, plm_matrix_np)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plm_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# ============ 8) Explication avec SHAP ============\n",
    "def model_wrapper(data):\n",
    "    # Conversion des donn√©es NumPy en tenseurs PyTorch\n",
    "    data_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "    plm_indices_batch = [(row > 0).nonzero(as_tuple=True)[0] for row in data_tensor] \n",
    "    with torch.no_grad():\n",
    "        return model(plm_indices_batch).cpu().numpy()\n",
    "\n",
    "# Conversion de la matrice PLM en tableau NumPy pour SHAP\n",
    "plm_matrix_np = plm_matrix.numpy()\n",
    "\n",
    "# Initialisation de l'explainer SHAP avec KernelExplainer\n",
    "explainer = shap.KernelExplainer(model_wrapper, plm_matrix_np)\n",
    "\n",
    "# Calcul des valeurs SHAP\n",
    "shap_values = explainer.shap_values(plm_matrix_np)\n",
    "\n",
    "# Visualisation des valeurs SHAP\n",
    "shap.summary_plot(shap_values, plm_matrix_np, feature_names=df_plm.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccba38a-2639-42b2-848e-bded3f5f737d",
   "metadata": {},
   "source": [
    "# ============ 9) Recherche d'Hyperparam√®tres avec Skorch ============\n",
    "skorch_model = NeuralNetClassifier(\n",
    "    module=GeneExpressionModel,\n",
    "    module__num_plms=plm_matrix.size(1),\n",
    "    module__num_classes=3,\n",
    "    max_epochs=10,\n",
    "    lr=1e-4,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "param_distributions = {\n",
    "    'lr': uniform(1e-5, 1e-2),\n",
    "    'max_epochs': randint(10, 50),\n",
    "    'batch_size': randint(16, 64),\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(skorch_model, param_distributions, n_iter=20, scoring='accuracy', cv=3, random_state=42)\n",
    "X, y = plm_matrix.numpy(), labels.numpy().flatten()\n",
    "search.fit(X, y)\n",
    "print(\"Meilleurs param√®tres:\", search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
